{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Spark Broker\n",
    "\n",
    "author: **Julien Peloton** [@JulienPeloton](https://github.com/JulienPeloton)  \n",
    "Last Verifed to Run: 2019-02-01\n",
    "\n",
    "Welcome to the first part of this broker bootcamp!\n",
    "The purpose of this notebook is to test Spark Streaming capability in the context of the DESC Broker design. \n",
    "\n",
    "**Useful Links:**\n",
    "\n",
    "* https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "* https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\n",
    "* https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html\n",
    "* https://docs.databricks.com/_static/notebooks/structured-streaming-python.html\n",
    "* https://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Running the LSST alert system\n",
    "\n",
    "In order to play with this notebook, you need to create the stream of alerts. This is handled by the [lsst-dm/alert_stream](https://github.com/lsst-dm/alert_stream) repository, maintained by the LSST DM group. Here are the steps you need:\n",
    "\n",
    "```bash\n",
    "# clone the repo\n",
    "git clone https://github.com/lsst-dm/alert_stream.git\n",
    "cd alert_stream\n",
    "\n",
    "# Launch Zookeeper and Kafka servers\n",
    "docker-compose up -d\n",
    "\n",
    "# Build the alert stream image\n",
    "docker build -t \"alert_stream\" .\n",
    "\n",
    "# Send bursts of alerts at expected visit intervals to topic \"my-stream\":\n",
    "docker run -it --rm \\\n",
    "    --network=alert_stream_default \\\n",
    "    -v $PWD/data:/home/alert_stream/data:ro \\\n",
    "    alert_stream python bin/sendAlertStream.py kafka:9092 my-stream\n",
    "```\n",
    "\n",
    "At this stage the stream is created, and 4 alerts will be sent at ~30 seconds interval. No worry if it finishes before you started working, you will be able to consume them on a later time (and you can always relaunch the stream)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connecting to the stream with Apache Spark\n",
    "\n",
    "Once the stream is produced, you want to connect to it and read messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Grab the running Spark Session, \n",
    "# otherwise create it.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialise a DataFrame that will read the stream from Kafka. Note that `kafka.bootstrap.servers` and `subscribe` must correspond to the arguments used for the LSST alert system (see above). At this point, we just create a Kafka Source for Streaming Queries - nothing really happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DF from the incoming stream from Kafka\n",
    "# Note that <kafka.bootstrap.servers> and <subscribe>\n",
    "# must correspond to arguments of the LSST alert system.\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"my-stream\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "\n",
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `df` is a streaming Dataframe. You can start streaming computation, by defining the sink and starting it. \n",
    "<!-- In our case, we want to interactively query the counts (same queries as above), so we will set the complete set of 1 hour counts to be in a in-memory table (note that this for testing purpose only in Spark 2.0). -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: First interaction with the stream\n",
    "\n",
    "Let's see first if we can receive correctly alert packets. For this, we define a query to the stream that will just count in real-time the number of alerts received by Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# We group data by partitions, \n",
    "# and count the number of alerts per partition\n",
    "# every 30 seconds.\n",
    "streamingCountsDF = (\n",
    "  df.groupBy(\n",
    "      \"partition\", \n",
    "      window(\"timestamp\", \"30 second\"))\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the size of shuffles small\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  \n",
    "\n",
    "# Trig`ger the streaming computation, \n",
    "# by defining the sink (memory here) and starting it\n",
    "countQuery = streamingCountsDF \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"qcount\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`qcount` is a handle to the streaming query that is running in the background. This query is continuously picking up alerts and updating the counts. You can easily access information about the progress in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick up recent progress\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "\n",
    "rp = countQuery.recentProgress\n",
    "print(rp[0])\n",
    "\n",
    "row_input_persec = [rp[i][\"sources\"][0][\"inputRowsPerSecond\"] for i in range(1, len(rp))]\n",
    "row_processed_persec = [rp[i][\"sources\"][0][\"processedRowsPerSecond\"] for i in range(1, len(rp))]\n",
    "time = [rp[i][\"batchId\"] for i in range(1, len(rp))]\n",
    "num_row = [rp[i][\"numInputRows\"] for i in range(1, len(rp))]\n",
    "\n",
    "fig = pl.figure(figsize=(10, 7))\n",
    "pl.plot(time, row_input_persec, label=\"Input rate: {:.2f} alerts/sec\".format(np.mean(row_input_persec)))\n",
    "pl.plot(time, row_processed_persec, label=\"Processed rate: {:.2f} alerts/sec\".format(np.mean(row_processed_persec)))\n",
    "pl.plot(time, num_row, label=\"Input alerts\")\n",
    "pl.ylim(0, 1500)\n",
    "pl.legend()\n",
    "pl.show()\n",
    "\n",
    "fig = pl.figure(figsize=(10, 7))\n",
    "\n",
    "mask = np.where(np.array(row_processed_persec) > 0)[0]\n",
    "speed = np.array([i/j for i, j in zip(np.array(num_row)[mask], np.array(row_processed_persec)[mask])])\n",
    "print(np.array(row_processed_persec)[mask])\n",
    "pl.plot(np.array(time)[mask], speed, label=\"Batch time: {:.2f} sec ({:.2f} events/batch)\".format(\n",
    "    np.mean(speed), \n",
    "    np.mean(np.array(num_row)[mask])))\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# al = spark.sql(\n",
    "#     \"select partition, date_format(window.end, 'YYYY-MMM-dd HH:mm:ss') as time, count from qcount order by time, partition\")\n",
    "al = spark.sql(\n",
    "    \"select partition, date_format(window.end, 'HH:mm:ss') as time, count from qcount order by time, partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al.withColumn(\"rate (events/seconds)\", rate(al[\"count\"], 30.)).show()\n",
    "pandas_df = al.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure(figsize=(10, 7))\n",
    "times = [int(i.replace(\":\", \"\")) for i in pandas_df[\"time\"]]\n",
    "pl.plot(times, pandas_df[\"count\"])\n",
    "pl.xticks(times, pandas_df[\"time\"][:])\n",
    "pl.gcf().autofmt_xdate()\n",
    "print(pandas_df[\"count\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Digging the alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawQuery = df\\\n",
    "    .writeStream \\\n",
    "    .queryName(\"qraw\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = spark.sql(\"select * from qalerts\")\n",
    "al.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = spark.sql(\"select * from qraw\")\n",
    "raw.show()\n",
    "raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_files = [\"../../sample-avro-alert/schema/diasource.avsc\",\n",
    "                    \"../../sample-avro-alert/schema/diaobject.avsc\",\n",
    "                    \"../../sample-avro-alert/schema/ssobject.avsc\",\n",
    "                    \"../../sample-avro-alert/schema/cutout.avsc\",\n",
    "                    \"../../sample-avro-alert/schema/alert.avsc\"]\n",
    "import fastavro\n",
    "import avro.schema\n",
    "import json\n",
    "def loadSingleAvsc(file_path, names):\n",
    "    \"\"\"Load a single avsc file.\n",
    "    \"\"\"\n",
    "    with open(file_path) as file_text:\n",
    "        json_data = json.load(file_text)\n",
    "    schema = avro.schema.SchemaFromJSONData(json_data, names)\n",
    "    return schema\n",
    "def combineSchemas(schema_files):\n",
    "    \"\"\"Combine multiple nested schemas into a single schema.\n",
    "    \"\"\"\n",
    "    known_schemas = avro.schema.Names()\n",
    "\n",
    "    for s in schema_files:\n",
    "        schema = loadSingleAvsc(s, known_schemas)\n",
    "    return schema.to_json()\n",
    "schema = combineSchemas(schema_files)\n",
    "import io\n",
    "\n",
    "def decoder(msg):\n",
    "    bytes_io = io.BytesIO(msg)\n",
    "    bytes_io.seek(0)\n",
    "    alert = fastavro.schemaless_reader(bytes_io, schema)\n",
    "    return alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw.take(1)\n",
    "val = data[0][\"value\"]\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, pandas_udf\n",
    "# from pyspark.sql.types import json\n",
    "# decode = pandas_udf(decoder, returnType=)\n",
    "rdd = raw.rdd.map(lambda x: decoder(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeRADecPart(part):\n",
    "    data = [*part]\n",
    "    alert = [i[\"alertId\"] for i in data]\n",
    "    ra = [i[\"diaSource\"][\"ra\"] for i in data]\n",
    "    dec = [i[\"diaSource\"][\"decl\"] for i in data]\n",
    "    \n",
    "    yield alert, ra, dec\n",
    "def takeRADec(dic):\n",
    "    alert = dic[\"alertId\"]\n",
    "    ra = dic[\"diaSource\"][\"ra\"]\n",
    "    dec = dic[\"diaSource\"][\"decl\"]\n",
    "    \n",
    "    return (alert, ra, dec)\n",
    "df = rdd.map(takeRADec).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "dic = {\"toto\":1, \"tata\":2}\n",
    "r = Row(**decoder(val))\n",
    "print(r)\n",
    "\n",
    "def as_row(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        dictionary = {k: as_row(v) for k, v in obj.items()}\n",
    "        return Row(**dictionary)\n",
    "    elif isinstance(obj, list):\n",
    "        return [as_row(v) for v in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def makeRow(dic):\n",
    "    myRow = Row(**dic)\n",
    "#     myRow = as_row(dic)\n",
    "    return myRow\n",
    "\n",
    "df2 = rdd.map(makeRow)\n",
    "# schemal = [(i, j) for i, j in zip(schema.keys(), schema.values())]\n",
    "# df2 = spark.createDataFrame(rdd.map(makeRow), schemal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i[\"type\"] for i in schema[\"fields\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Use pandas_udf to define a Pandas UDF\n",
    "@pandas_udf('double')\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "def decoderudf(msg):\n",
    "    bytes_io = io.BytesIO(msg)\n",
    "    bytes_io.seek(0)\n",
    "    alert = fastavro.schemaless_reader(bytes_io, schema)\n",
    "    return alert\n",
    "\n",
    "raw.withColumn(\"decoded\", decoderudf(raw[\"value\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, LongType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import udf, explode, split, from_json\n",
    "\n",
    "dec = udf(decoder, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdec = raw.withColumn(\"decoded\", dec(raw[\"value\"])).select(\"decoded\")\n",
    "dfdec.show()\n",
    "dfdec.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def str2dic(s):\n",
    "    return literal_eval(s)\n",
    "    \n",
    "str2dic_udf = udf(str2dic)\n",
    "# dfdec.select(\"decoded\", str2dic_udf(dfdec[\"decoded\"])).show()\n",
    "dfdec.rdd.map(lambda x: x.asDict()).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: cross-correlating with other catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
